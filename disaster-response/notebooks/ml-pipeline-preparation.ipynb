{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "This notebook was used to set up to try the machine learning pipeline of the disaster recovery project and to find the best model based on the results from the grid search with cross validation for multiple parameters and algorithms. The final model will be created in the python script train_classifier.py file, which is located [here](https://github.com/bruno-f7s/portfolio/tree/main/disaster-response/models/train_classifier.py).\n",
    "\n",
    "This notebook is not necessary for launching the app, but can be revisited if a new dataset needs to be used to train a new model.\n",
    "\n",
    "The code below is divided into the following steps:\n",
    "1. Import libraries\n",
    "2. Load and prepare the data\n",
    "3. Define NLP processing functions\n",
    "5. Build and run a machine learning pipeline\n",
    "6. Model choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bfernandes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download(['wordnet'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "conn = sqlite3.connect('DisasterResponse.db')\n",
    "query = \"SELECT * FROM DisasterResponse\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# remove rows which do not have any label as they will not provide any prediction\n",
    "label_cols = df.drop([\"id\",\"message\",\"original\",\"genre\",\"related\",\"child_alone\"], axis=1).columns\n",
    "df['sum'] = df[label_cols].sum(axis=1)\n",
    "df = df[df['sum'] != 0]\n",
    "df = df.drop([\"sum\"], axis=1)\n",
    "\n",
    "# create X and y variables\n",
    "X = df[\"message\"]\n",
    "y = df.drop([\"id\",\"message\",\"original\",\"genre\",\"related\",\"child_alone\"], axis=1)\n",
    "\n",
    "# create the train and test splits\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: request\n",
      "Percentage of 0s: 69.7%\n",
      "Percentage of 1s: 30.3%\n",
      "---------------------\n",
      "Label: offer\n",
      "Percentage of 0s: 99.2%\n",
      "Percentage of 1s: 0.8%\n",
      "---------------------\n",
      "Label: aid_related\n",
      "Percentage of 0s: 26.5%\n",
      "Percentage of 1s: 73.5%\n",
      "---------------------\n",
      "Label: medical_help\n",
      "Percentage of 0s: 85.9%\n",
      "Percentage of 1s: 14.1%\n",
      "---------------------\n",
      "Label: medical_products\n",
      "Percentage of 0s: 91.1%\n",
      "Percentage of 1s: 8.9%\n",
      "---------------------\n",
      "Label: search_and_rescue\n",
      "Percentage of 0s: 95.1%\n",
      "Percentage of 1s: 4.9%\n",
      "---------------------\n",
      "Label: security\n",
      "Percentage of 0s: 96.8%\n",
      "Percentage of 1s: 3.2%\n",
      "---------------------\n",
      "Label: military\n",
      "Percentage of 0s: 94.2%\n",
      "Percentage of 1s: 5.8%\n",
      "---------------------\n",
      "Label: water\n",
      "Percentage of 0s: 88.7%\n",
      "Percentage of 1s: 11.3%\n",
      "---------------------\n",
      "Label: food\n",
      "Percentage of 0s: 80.2%\n",
      "Percentage of 1s: 19.8%\n",
      "---------------------\n",
      "Label: shelter\n",
      "Percentage of 0s: 84.3%\n",
      "Percentage of 1s: 15.7%\n",
      "---------------------\n",
      "Label: clothing\n",
      "Percentage of 0s: 97.3%\n",
      "Percentage of 1s: 2.7%\n",
      "---------------------\n",
      "Label: money\n",
      "Percentage of 0s: 95.9%\n",
      "Percentage of 1s: 4.1%\n",
      "---------------------\n",
      "Label: missing_people\n",
      "Percentage of 0s: 98.0%\n",
      "Percentage of 1s: 2.0%\n",
      "---------------------\n",
      "Label: refugees\n",
      "Percentage of 0s: 94.1%\n",
      "Percentage of 1s: 5.9%\n",
      "---------------------\n",
      "Label: death\n",
      "Percentage of 0s: 91.9%\n",
      "Percentage of 1s: 8.1%\n",
      "---------------------\n",
      "Label: other_aid\n",
      "Percentage of 0s: 76.7%\n",
      "Percentage of 1s: 23.3%\n",
      "---------------------\n",
      "Label: infrastructure_related\n",
      "Percentage of 0s: 88.5%\n",
      "Percentage of 1s: 11.5%\n",
      "---------------------\n",
      "Label: transport\n",
      "Percentage of 0s: 91.9%\n",
      "Percentage of 1s: 8.1%\n",
      "---------------------\n",
      "Label: buildings\n",
      "Percentage of 0s: 91.0%\n",
      "Percentage of 1s: 9.0%\n",
      "---------------------\n",
      "Label: electricity\n",
      "Percentage of 0s: 96.4%\n",
      "Percentage of 1s: 3.6%\n",
      "---------------------\n",
      "Label: tools\n",
      "Percentage of 0s: 98.9%\n",
      "Percentage of 1s: 1.1%\n",
      "---------------------\n",
      "Label: hospitals\n",
      "Percentage of 0s: 98.1%\n",
      "Percentage of 1s: 1.9%\n",
      "---------------------\n",
      "Label: shops\n",
      "Percentage of 0s: 99.2%\n",
      "Percentage of 1s: 0.8%\n",
      "---------------------\n",
      "Label: aid_centers\n",
      "Percentage of 0s: 97.9%\n",
      "Percentage of 1s: 2.1%\n",
      "---------------------\n",
      "Label: other_infrastructure\n",
      "Percentage of 0s: 92.2%\n",
      "Percentage of 1s: 7.8%\n",
      "---------------------\n",
      "Label: weather_related\n",
      "Percentage of 0s: 50.7%\n",
      "Percentage of 1s: 49.3%\n",
      "---------------------\n",
      "Label: floods\n",
      "Percentage of 0s: 85.4%\n",
      "Percentage of 1s: 14.6%\n",
      "---------------------\n",
      "Label: storm\n",
      "Percentage of 0s: 83.5%\n",
      "Percentage of 1s: 16.5%\n",
      "---------------------\n",
      "Label: fire\n",
      "Percentage of 0s: 98.1%\n",
      "Percentage of 1s: 1.9%\n",
      "---------------------\n",
      "Label: earthquake\n",
      "Percentage of 0s: 83.4%\n",
      "Percentage of 1s: 16.6%\n",
      "---------------------\n",
      "Label: cold\n",
      "Percentage of 0s: 96.4%\n",
      "Percentage of 1s: 3.6%\n",
      "---------------------\n",
      "Label: other_weather\n",
      "Percentage of 0s: 90.7%\n",
      "Percentage of 1s: 9.3%\n",
      "---------------------\n",
      "Label: direct_report\n",
      "Percentage of 0s: 65.7%\n",
      "Percentage of 1s: 34.3%\n",
      "---------------------\n",
      "Total number of rows: 14805\n"
     ]
    }
   ],
   "source": [
    "# print the distribution of the values for each label\n",
    "for column in label_cols:\n",
    "    print(f\"Label: {column}\")\n",
    "    print(f\"Percentage of 0s: {round((len(y[y[column] == 0])/len(y[column]))*100, 1)}%\")\n",
    "    print(f\"Percentage of 1s: {round((len(y[y[column] == 1])/len(y[column]))*100, 1)}%\")\n",
    "    print(f\"---------------------\")\n",
    "    \n",
    "print(f\"Total number of rows: {len(df)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation 1__: In this project a train and validation splits were created __without the holdout set__ (which I typically do) for a few reasons:\n",
    "1. This is a multi-label problem with 36 labels so I decided it was feasible to use directly all the data available because there are some labels which are highly imbalanced and this could lead to many missing data of one category during training.\n",
    "2. The dataset is not that big and creating this holdout set would remove more data from the training processed.\n",
    "3. This code is intended to be used to analyze and decide for the best model but the final model will be created using all data nonetheless. Therefore the results of the train/validation splits are sufficient to make a decision.\n",
    "\n",
    "__Observation 2__: The label categories \"request\" and \"child_alone\" only have one category available in this dataset (see distribution above). Because of this the model will always predict the same label regardless of the input. I decided to __remove them__ because they are simply adding noise to the model and do not impact the predictions in any positive way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define NLP processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenize function that processes text data\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes in a string, tokenizes it, removes English stop words and applies lemmatization. \n",
    "    The result is a list of all resulting tokens. This function can be passed into a tokenizer of a transformer.\n",
    "    \"\"\"\n",
    "    #tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #stop word removal\n",
    "    tokens = [tok.lower().strip() for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "\n",
    "    #lemmatization of words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(tok, pos='v') for tok in tokens]\n",
    "\n",
    "    #remove tokens that only contain special characters\n",
    "    clean_tokens = [tok for tok in lemmatized_tokens if not re.match(\"^[\\W_]+\", tok)]\n",
    "    \n",
    "    #remove tokens that contain digits as they will not be relevant for this supervised learning problem\n",
    "    clean_tokens = [tok for tok in clean_tokens if not re.search(\"\\d\", tok)]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the Hurricane over or is it not over\n",
      "['be', 'hurricane'] \n",
      "\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "['un', 'report', 'leogane', 'destroy', 'only', 'hospital', 'st.', 'croix', 'function', 'need', 'supply', 'desperately'] \n",
      "\n",
      "Storm at sacred heart of jesus\n",
      "['storm', 'sacred', 'heart', 'jesus'] \n",
      "\n",
      "Please, we need tents and water. We are in Silo, Thank you!\n",
      "['please', 'need', 'tent', 'water', 'we', 'silo', 'thank'] \n",
      "\n",
      "I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets )\n",
      "['i', 'croix-des-bouquets', 'we', 'health', 'issue', 'they', 'workers', 'santo', 'area', 'croix-des-bouquets'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the results of the tokenize function\n",
    "for message in X[:5]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom transformer to count words\n",
    "class WordCounter:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [[len(text.split())] for text in X]\n",
    "\n",
    "# create a custom transformer to measure length of message\n",
    "class CharacterCounter:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [[len(text)] for text in X]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and run a machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model Logistic Regression using grid search with cross-validation...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Model: Logistic Regression\n",
      "Best Parameters: {'model__estimator__estimator__C': 1, 'model__estimator__estimator__class_weight': 'balanced', 'model__estimator__estimator__l1_ratio': 0, 'model__estimator__estimator__multi_class': 'ovr', 'model__estimator__estimator__n_jobs': -1, 'model__estimator__estimator__penalty': 'elasticnet', 'model__estimator__estimator__random_state': 42, 'model__estimator__estimator__solver': 'saga'}\n",
      "Best Score: 0.4278361162311718\n",
      "Overall accuracy: request                   0.803520\n",
      "offer                     0.962546\n",
      "aid_related               0.626484\n",
      "medical_help              0.634875\n",
      "medical_products          0.706508\n",
      "search_and_rescue         0.770774\n",
      "security                  0.813754\n",
      "military                  0.774048\n",
      "water                     0.634056\n",
      "food                      0.646541\n",
      "shelter                   0.610929\n",
      "clothing                  0.858985\n",
      "money                     0.803111\n",
      "missing_people            0.881907\n",
      "refugees                  0.750307\n",
      "death                     0.696275\n",
      "other_aid                 0.567131\n",
      "infrastructure_related    0.670487\n",
      "transport                 0.715718\n",
      "buildings                 0.702415\n",
      "electricity               0.835039\n",
      "tools                     0.940442\n",
      "hospitals                 0.891731\n",
      "shops                     0.946582\n",
      "aid_centers               0.883135\n",
      "other_infrastructure      0.732296\n",
      "weather_related           0.641220\n",
      "floods                    0.659844\n",
      "storm                     0.727384\n",
      "fire                      0.885591\n",
      "earthquake                0.746828\n",
      "cold                      0.831764\n",
      "other_weather             0.692796\n",
      "direct_report             0.753377\n",
      "dtype: float64\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               request       0.65      0.78      0.71      1481\n",
      "                 offer       0.03      0.10      0.04        41\n",
      "           aid_related       0.78      0.68      0.73      3597\n",
      "          medical_help       0.16      0.37      0.23       701\n",
      "      medical_products       0.10      0.29      0.14       420\n",
      "     search_and_rescue       0.05      0.20      0.08       235\n",
      "              security       0.04      0.19      0.07       166\n",
      "              military       0.10      0.43      0.16       254\n",
      "                 water       0.13      0.43      0.20       514\n",
      "                  food       0.29      0.56      0.38       963\n",
      "               shelter       0.19      0.47      0.27       741\n",
      "              clothing       0.07      0.34      0.12       135\n",
      "                 money       0.05      0.23      0.09       195\n",
      "        missing_people       0.02      0.12      0.04        98\n",
      "              refugees       0.07      0.29      0.11       273\n",
      "                 death       0.09      0.32      0.14       387\n",
      "             other_aid       0.26      0.48      0.34      1122\n",
      "infrastructure_related       0.13      0.35      0.19       527\n",
      "             transport       0.10      0.33      0.16       389\n",
      "             buildings       0.10      0.30      0.15       432\n",
      "           electricity       0.05      0.21      0.09       181\n",
      "                 tools       0.02      0.08      0.03        52\n",
      "             hospitals       0.01      0.07      0.02        81\n",
      "                 shops       0.00      0.03      0.01        37\n",
      "           aid_centers       0.02      0.09      0.03        99\n",
      "  other_infrastructure       0.10      0.32      0.15       360\n",
      "       weather_related       0.65      0.60      0.63      2437\n",
      "                floods       0.24      0.56      0.33       740\n",
      "                 storm       0.31      0.51      0.39       834\n",
      "                  fire       0.02      0.10      0.03        77\n",
      "            earthquake       0.33      0.48      0.39       820\n",
      "                  cold       0.07      0.27      0.11       183\n",
      "         other_weather       0.12      0.38      0.18       446\n",
      "         direct_report       0.62      0.73      0.67      1695\n",
      "\n",
      "             micro avg       0.26      0.53      0.35     20713\n",
      "             macro avg       0.18      0.34      0.22     20713\n",
      "          weighted avg       0.41      0.53      0.44     20713\n",
      "           samples avg       0.29      0.55      0.35     20713\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Training the model Random Forest using grid search with cross-validation...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Model: Random Forest\n",
      "Best Parameters: {'model__estimator__class_weight': 'balanced', 'model__estimator__max_depth': 5, 'model__estimator__n_estimators': 100, 'model__estimator__random_state': 42}\n",
      "Best Score: 0.43680547118601687\n",
      "Overall accuracy: request                   0.795538\n",
      "offer                     0.990381\n",
      "aid_related               0.681539\n",
      "medical_help              0.676013\n",
      "medical_products          0.804748\n",
      "search_and_rescue         0.846910\n",
      "security                  0.940237\n",
      "military                  0.649406\n",
      "water                     0.676627\n",
      "food                      0.636717\n",
      "shelter                   0.558330\n",
      "clothing                  0.905853\n",
      "money                     0.900941\n",
      "missing_people            0.975235\n",
      "refugees                  0.778756\n",
      "death                     0.811298\n",
      "other_aid                 0.578797\n",
      "infrastructure_related    0.701392\n",
      "transport                 0.708146\n",
      "buildings                 0.771183\n",
      "electricity               0.905853\n",
      "tools                     0.988334\n",
      "hospitals                 0.977282\n",
      "shops                     0.992223\n",
      "aid_centers               0.965207\n",
      "other_infrastructure      0.728817\n",
      "weather_related           0.627916\n",
      "floods                    0.583709\n",
      "storm                     0.764020\n",
      "fire                      0.956815\n",
      "earthquake                0.823373\n",
      "cold                      0.876791\n",
      "other_weather             0.659844\n",
      "direct_report             0.756242\n",
      "dtype: float64\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               request       0.65      0.69      0.67      1481\n",
      "                 offer       0.12      0.02      0.04        41\n",
      "           aid_related       0.78      0.79      0.79      3597\n",
      "          medical_help       0.19      0.38      0.25       701\n",
      "      medical_products       0.09      0.13      0.10       420\n",
      "     search_and_rescue       0.05      0.13      0.07       235\n",
      "              security       0.06      0.05      0.05       166\n",
      "              military       0.10      0.69      0.17       254\n",
      "                 water       0.14      0.41      0.21       514\n",
      "                  food       0.29      0.59      0.39       963\n",
      "               shelter       0.19      0.59      0.29       741\n",
      "              clothing       0.10      0.30      0.15       135\n",
      "                 money       0.06      0.11      0.08       195\n",
      "        missing_people       0.04      0.01      0.02        98\n",
      "              refugees       0.08      0.27      0.12       273\n",
      "                 death       0.11      0.19      0.14       387\n",
      "             other_aid       0.26      0.45      0.33      1122\n",
      "infrastructure_related       0.14      0.34      0.20       527\n",
      "             transport       0.11      0.38      0.17       389\n",
      "             buildings       0.10      0.20      0.13       432\n",
      "           electricity       0.07      0.13      0.09       181\n",
      "                 tools       0.00      0.00      0.00        52\n",
      "             hospitals       0.03      0.01      0.02        81\n",
      "                 shops       0.00      0.00      0.00        37\n",
      "           aid_centers       0.00      0.00      0.00        99\n",
      "  other_infrastructure       0.11      0.38      0.17       360\n",
      "       weather_related       0.62      0.64      0.63      2437\n",
      "                floods       0.22      0.71      0.34       740\n",
      "                 storm       0.34      0.40      0.37       834\n",
      "                  fire       0.03      0.05      0.04        77\n",
      "            earthquake       0.47      0.36      0.41       820\n",
      "                  cold       0.06      0.14      0.08       183\n",
      "         other_weather       0.13      0.46      0.20       446\n",
      "         direct_report       0.64      0.68      0.66      1695\n",
      "\n",
      "             micro avg       0.31      0.53      0.39     20713\n",
      "             macro avg       0.19      0.31      0.22     20713\n",
      "          weighted avg       0.41      0.53      0.45     20713\n",
      "           samples avg       0.34      0.56      0.39     20713\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Training the model Gradient Boosting using grid search with cross-validation...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gradient Boosting\n",
      "Best Parameters: {'model__estimator__learning_rate': 0.1, 'model__estimator__max_depth': 7, 'model__estimator__n_estimators': 100}\n",
      "Best Score: 0.3363166671615735\n",
      "Overall accuracy: request                   0.802088\n",
      "offer                     0.985878\n",
      "aid_related               0.732910\n",
      "medical_help              0.852436\n",
      "medical_products          0.910970\n",
      "search_and_rescue         0.947401\n",
      "security                  0.953745\n",
      "military                  0.945354\n",
      "water                     0.892755\n",
      "food                      0.793901\n",
      "shelter                   0.841588\n",
      "clothing                  0.960295\n",
      "money                     0.955792\n",
      "missing_people            0.963979\n",
      "refugees                  0.941056\n",
      "death                     0.919771\n",
      "other_aid                 0.762792\n",
      "infrastructure_related    0.889480\n",
      "transport                 0.919975\n",
      "buildings                 0.909537\n",
      "electricity               0.957634\n",
      "tools                     0.983422\n",
      "hospitals                 0.969095\n",
      "shops                     0.985673\n",
      "aid_centers               0.968686\n",
      "other_infrastructure      0.923250\n",
      "weather_related           0.628940\n",
      "floods                    0.843635\n",
      "storm                     0.837086\n",
      "fire                      0.967867\n",
      "earthquake                0.855096\n",
      "cold                      0.958043\n",
      "other_weather             0.906263\n",
      "direct_report             0.753786\n",
      "dtype: float64\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               request       0.71      0.59      0.64      1481\n",
      "                 offer       0.03      0.02      0.03        41\n",
      "           aid_related       0.75      0.95      0.84      3597\n",
      "          medical_help       0.12      0.00      0.01       701\n",
      "      medical_products       0.11      0.00      0.01       420\n",
      "     search_and_rescue       0.11      0.01      0.02       235\n",
      "              security       0.06      0.02      0.03       166\n",
      "              military       0.16      0.01      0.02       254\n",
      "                 water       0.22      0.01      0.02       514\n",
      "                  food       0.32      0.04      0.07       963\n",
      "               shelter       0.13      0.01      0.02       741\n",
      "              clothing       0.03      0.01      0.02       135\n",
      "                 money       0.04      0.01      0.01       195\n",
      "        missing_people       0.04      0.03      0.03        98\n",
      "              refugees       0.06      0.00      0.01       273\n",
      "                 death       0.27      0.01      0.02       387\n",
      "             other_aid       0.21      0.01      0.02      1122\n",
      "infrastructure_related       0.22      0.01      0.02       527\n",
      "             transport       0.44      0.02      0.03       389\n",
      "             buildings       0.08      0.00      0.00       432\n",
      "           electricity       0.04      0.01      0.01       181\n",
      "                 tools       0.00      0.00      0.00        52\n",
      "             hospitals       0.01      0.01      0.01        81\n",
      "                 shops       0.00      0.00      0.00        37\n",
      "           aid_centers       0.03      0.02      0.03        99\n",
      "  other_infrastructure       0.17      0.01      0.02       360\n",
      "       weather_related       0.66      0.52      0.58      2437\n",
      "                floods       0.20      0.01      0.02       740\n",
      "                 storm       0.59      0.15      0.23       834\n",
      "                  fire       0.02      0.03      0.02        77\n",
      "            earthquake       0.74      0.21      0.33       820\n",
      "                  cold       0.04      0.01      0.01       183\n",
      "         other_weather       0.25      0.01      0.03       446\n",
      "         direct_report       0.67      0.57      0.62      1695\n",
      "\n",
      "             micro avg       0.65      0.34      0.44     20713\n",
      "             macro avg       0.22      0.10      0.11     20713\n",
      "          weighted avg       0.45      0.34      0.34     20713\n",
      "           samples avg       0.68      0.37      0.44     20713\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Models sucessfully trained!\n",
      "Time it took for training: 2 days, 3:47:49.053763\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# BUILD A ML PIPELINE\n",
    "#########################################################################\n",
    "modelling_start_time = datetime.datetime.now()\n",
    "\n",
    "# Create a dictionary of ml models with their respective hyperparameters\n",
    "models = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": MultiOutputClassifier(OneVsRestClassifier(LogisticRegression())),\n",
    "        \"param_grid\": {\n",
    "            'model__estimator__estimator__penalty': ['elasticnet'],\n",
    "            'model__estimator__estimator__l1_ratio': [0, 0.5, 1],\n",
    "            'model__estimator__estimator__C': [1, 10, 25],\n",
    "            'model__estimator__estimator__solver': ['saga'],\n",
    "            'model__estimator__estimator__multi_class': ['ovr'],\n",
    "            'model__estimator__estimator__class_weight': ['balanced'],\n",
    "            'model__estimator__estimator__random_state': [42],\n",
    "            'model__estimator__estimator__n_jobs': [-1]\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": MultiOutputClassifier(RandomForestClassifier()),\n",
    "        \"param_grid\": {\n",
    "            'model__estimator__n_estimators': [1, 10, 100],\n",
    "            'model__estimator__max_depth': [1, 5, 10],\n",
    "            'model__estimator__class_weight': ['balanced'],\n",
    "            'model__estimator__random_state': [42]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": MultiOutputClassifier(GradientBoostingClassifier()),\n",
    "        \"param_grid\": {\n",
    "            \"model__estimator__n_estimators\": [10, 50, 100],\n",
    "            \"model__estimator__learning_rate\": [0.1, 0.05, 0.01],\n",
    "            \"model__estimator__max_depth\": [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=1),\n",
    "    'f1': make_scorer(f1_score, average='weighted', zero_division=1)\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate over the models and perform grid search with cross-validation\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"Training the model {model_name} using grid search with cross-validation...\")\n",
    "    \n",
    "    # Define the pipeline with preprocessing and model\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=tokenize)),\n",
    "            ('token_count', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('word_count', Pipeline([\n",
    "                ('count', WordCounter()),\n",
    "                ('scale', StandardScaler())\n",
    "            ])),\n",
    "            ('character_count', Pipeline([\n",
    "                ('count', CharacterCounter()),\n",
    "                ('scale', StandardScaler())\n",
    "            ]))\n",
    "        ])),\n",
    "        ('model', model_config['model'])\n",
    "    ])\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(pipeline, \n",
    "                               param_grid=model_config[\"param_grid\"], \n",
    "                               cv=5, \n",
    "                               scoring=scoring, \n",
    "                               return_train_score=True,\n",
    "                               refit=\"f1\",\n",
    "                               verbose=1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model and its evaluation metric score\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    # Predict on the test set and evaluate the performance\n",
    "    y_pred = best_model.predict(X_validation)\n",
    "    \n",
    "    # Evaluate the overall accuracy\n",
    "    accuracy = (y_validation == y_pred).mean()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Score: {best_score}\")\n",
    "    print(f\"Overall accuracy: {accuracy}\")\n",
    "    print(classification_report(y_validation, y_pred, target_names=y.columns, zero_division=1))\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "modelling_end_time = datetime.datetime.now()\n",
    "\n",
    "print(f\"Models sucessfully trained!\")\n",
    "print(f\"Time it took for training: {str(modelling_end_time - modelling_start_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model choice\n",
    "The purpose of this project was not to achieve a specific performance metric but rather create a ml-pipeline that easily takes care of an NLP task that is then fed into an app. Nevertheless I decided to apply GridSearchCV to search for multiple parameter combinations for different algorithms. I also using different scoring methods but tuned for the best f1-score. I used this metric because the data has many labels that are very imbalanced. Accuracy would be a too simplistic metric and precision was highly inflated, so I tried to balance it with the recall using the f1-score.\n",
    "\n",
    "Based on these 3 algorithms and the parameter combination I chose the __logistic regression__ because it had the best balance of scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
